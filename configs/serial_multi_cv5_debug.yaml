seed: 0
exp_name: serial_multitask

device:
  cuda_device: cuda:0
  gpu_num_workers: 6

data:
  cv: 5

  model_dir: models/Multi/DBG # folder to save trained model
  report_dir: reports/Multi/DBG # folder to save results


  modes:
    morbidity:
      img:
        loader_name: custom_preprocessing
        img_dir: data/AIforCOVID # path for the dicom directory
        box_file: data/AIforCOVID/processed/box_data_AXF123.xlsx  # if equal to None do not select the bounding box
        mask_dir: data/AIforCOVID/processed/masks  # if equal to None do not select lung
        fold_dir: data/processed/AFC/5 # folder where upload the CV splits at patient level
      classes: # name of classes
        - MILD
        - SEVERE
    severity:
      img:
        loader_name: custom_preprocessing
        img_dir: data/BRIXIA # path for the dicom directory
        box_file: data/BRIXIA/processed/box_data_BX.xlsx  # if equal to None do not select the bounding box
        mask_dir: data/BRIXIA/processed/masks  # if equal to None do not select lung
        fold_dir: data/processed/BX/5 # folder where upload the CV splits at patient level
      classes: # name of classes
        - A
        - B
        - C
        - D
        - E
        - F
  img_dim: 256 # resize shape (square)
  preprocess:
    clahe: False
    filter: False
    clip: False
    masked: False
    bbox_resize: True

model:
  freezing: True
  model_name: multi
  task: multitask
  head: serial
  pretrained: [1,1] # if this parameter is None, loading the two models the first id is for AFC and the other one is for BX
  dropout_rate: 0.50 # dropout rate
  regression_type: area
  softmax: False # if True, the output is a probability distribution over the classes
  beta: 0.5 # weights balance between task AFC-BX
  train_backbone: True # if True, the backbone is trained

trainer:
  loss_1: mse # loss function
  loss_2: mse # loss function
  warmup_epochs: 1 # number of epochs to warm up the model, if 0 the model the optimization starts from the first epoch
  optimizer:
    lr: 0.001 # initial learning rate
    weight_decay: 0.00001 # weight decay
  scheduler: # allow to change learning rate during training
    mode: min
    patience: 20 # if the validation loss do not improve for x epochs, the scheduler reduces the learning rate (lr*10^-1)
    min_lr: 1e-7
    factor: 0.75 # if the validation loss do not improve for x epochs, the scheduler reduces the learning rate (lr*10^-1)
  early_stopping: 100 # if the validation loss do not improve for x epochs stop training
  max_epochs: 2
  batch_size: 64
  normalizer: ImageNet # normalization of the input images (Imagenet, Standardizer, MinMax, None)