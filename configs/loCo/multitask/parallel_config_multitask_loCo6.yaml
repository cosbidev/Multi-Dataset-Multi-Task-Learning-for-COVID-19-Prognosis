seed: 0
exp_name: parallel_multitask

device:
  cuda_device: cuda:0
  gpu_num_workers: 6

data:
  cv: loCo6
  model_dir: models/Multi/loCo6 # folder to save trained model
  report_dir: reports/Multi/loCo6 # folder to save results

  batch_size: 64
  modes:
    morbidity:
      img:
        loader_name: custom_preprocessing
        img_dir: data/AIforCOVID # path for the dicom directory
        box_file: data/AIforCOVID/processed/box_data_AXF123.xlsx  # if equal to None do not select the bounding box
        mask_dir: data/AIforCOVID/processed/masks  # if equal to None do not select lung
        fold_dir: data/processed/AFC/loCo # folder where upload the CV splits at patient level
      classes: # name of classes
        - MILD
        - SEVERE
    severity:
      img:
        loader_name: custom_preprocessing
        img_dir: data/BRIXIA # path for the dicom directory
        box_file: data/BRIXIA/processed/box_data_BX.xlsx  # if equal to None do not select the bounding box
        mask_dir: data/BRIXIA/processed/masks  # if equal to None do not select lung
        fold_dir: data/processed/BX/loCo # folder where upload the CV splits at patient level
      classes: # name of classes
        - A
        - B
        - C
        - D
        - E
        - F
  img_dim: 256 # resize shape (square)
  preprocess:
    clahe: False
    filter: False
    clip: False
    masked: False
    bbox_resize: True

model:
  freezing: False
  model_name: multi
  task: multitask
  head: parallel
  pretrained: False # if this parameter is None, the model is trained from scratch
  dropout_rate: 0.50 # dropout rate
  regression_type: area
  softmax: False # if True, the output is a probability distribution over the classes

trainer:
  loss_1: mse # loss function
  loss_2: mse # loss function
  warmup_epochs: 50 # number of epochs to warm up the model, if 0 the model the optimization starts from the first epoch
  optimizer:
    lr: 0.001 # initial learning rate
    weight_decay: 0.00001 # weight decay
  scheduler: # allow to change learning rate during training
    mode: min
    patience: 20 # if the validation loss do not improve for x epochs, the scheduler reduces the learning rate (lr*10^-1)
    min_lr: 1e-7
    factor: 0.75 # if the validation loss do not improve for x epochs, the scheduler reduces the learning rate (lr*10^-1)
  early_stopping: 100 # if the validation loss do not improve for x epochs stop training
  max_epochs: 300